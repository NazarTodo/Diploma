---
title: "Developing an ensemble approach for predicting customer churn"
output: html_notebook
author: Nazar Todoshchuk
---

```{r}
# In order to run the code, uncomment this chunk and install required packages

# install.packages('readxl')
# install.packages('caret')
# install.packages('corrplot')
# install.packages('outliers')
# install.packages('e1071')
# install.packages("randomForest")
# install.packages("rpart.plot")
# install.packages("xgboost")
# install.packages("SHAPforxgboost")
# install.packages("factoextra")
# install.packages("NbClust")
# install.packages("plotly")
```

```{r}
library("readxl")
library(corrplot)
library(outliers)
library(caret)
library(e1071)
library("randomForest")
library(rpart)
library(rpart.plot)
library(xgboost)
library("SHAPforxgboost")
library(factoextra)
library(NbClust)
library(clValid)
library(dplyr)
library(plotly)
```

```{r}
# Loading data
data <- read_excel("ChurnData.xlsx")
head(data)
```

```{r}
# Removing NA values
data <- data[,colSums(is.na(data))<nrow(data)]
data <- na.omit(data)
# Removing unnecessary column Customer
data <- subset(data, select = -Customer)
# Removing spaces in column names
colnames(data) <- make.names(names(data))
# Removing 0s in Account.Length column
data[data$Account.Length == 0, ] <- 1
```

```{r}
hist(data$Customer.Service.Calls,
    main = "Customer service calls distribution",
    xlab = "Customer service calls, calls",
    xlim = c(0, max(data$Customer.Service.Calls)),
    col = "dodgerblue3",
    breaks = 10)

axis(side=1, at=seq(0, 9, 1))
```

```{r}
# Churners VS Non churners distribution
num_of_churners_non_churners <- c(nrow(data[data$Churn==1, ]),nrow(data[data$Churn==0, ]))
percentage <- round(num_of_churners_non_churners * 100 / sum(num_of_churners_non_churners), 1)
colors <- c("dodgerblue1","dodgerblue4")

pie(num_of_churners_non_churners, 
    main = "Churners VS Non churners distribution",
    labels = percentage,
    col = colors)

legend("topright", c("Churners","Non churners"), cex = 1, fill = colors)

```

```{r}
par(mfrow=c(3,1))
# Income boxplot
boxplot(data$Income,
        scientific = FALSE,
        horizontal = TRUE,
        ylim = c(0, max(data$Income)),
        main = "Income distribution",
        xlab = "Annual Income, $",
        col="dodgerblue3")
# Account Length boxplot
boxplot(data$Account.Length,
        scientific = FALSE,
        horizontal = TRUE,
        ylim = c(0, max(data$Account.Length)),
        main = "Account Length distribution",
        xlab = "Account Length, months",
        col="dodgerblue3")
# Voice Mail Messages boxplot
boxplot(data$Voice.Mail.Messages,
        scientific = FALSE,
        horizontal = TRUE,
        ylim = c(0, max(data$Voice.Mail.Messages)),
        main = "Voice Mail Messages distribution",
        xlab = "Voice Mail Messages, messages",
        col="dodgerblue3")
```

```{r}
par(mfrow=c(3,1))
# Day Minutes boxplot
boxplot(data$Day.Minutes,
        scientific = FALSE,
        horizontal = TRUE,
        ylim = c(0, max(data$Day.Minutes)),
        main = "Day Minutes distribution",
        xlab = "Day Minutes, min",
        col="dodgerblue3")
# Evening Minutes boxplot
boxplot(data$Evening.Minutes,
        scientific = FALSE,
        horizontal = TRUE,
        ylim = c(0, max(data$Evening.Minutes)),
        main = "Evening Minutes distribution",
        xlab = "Evening Minutes, min",
        col="dodgerblue3")
# Night Minutes boxplot
boxplot(data$Night.Minutes,
        scientific = FALSE,
        horizontal = TRUE,
        ylim = c(0, max(data$Night.Minutes)),
        main = "Night Minutes distribution",
        xlab = "Night Minutes, min",
        col="dodgerblue3")
```

```{r}
# International Minutes boxplot
boxplot(data$International.Minutes,
        scientific = FALSE,
        horizontal = TRUE,
        ylim = c(0, max(data$International.Minutes)),
        main = "International Minutes distribution",
        xlab = "International Minutes, min",
        col="dodgerblue3")
```

```{r}
data_out <- data[, c("Income", "Account.Length", "Day.Minutes", "Day.Calls", "Evening.Minutes", "Evening.Calls", "Night.Minutes", "Night.Calls", "Customer.Service.Calls")]

# Filtering out outliers using z-score method
z_scores <- abs(scale(data_out))

not_outliers <- which(!rowSums(z_scores>3))

data <- data[rownames(data) %in% not_outliers, ]
```

```{r}
# Building the correlation plot to see if there is correlation between columns
corrplot(cor(data),
         method = "circle",
         type = "lower",
         tl.col = "black",
         tl.cex = 0.6,
         bg = "white",
         mar=c(0,0,2,0))
```

```{r}
# Calculating monthly day, evening and night calls durations
data$dcall_avg <- round(data$Day.Minutes / data$Account.Length, 3)
data$ecall_avg <- round(data$Evening.Minutes / data$Account.Length, 3)
data$ncall_avg <- round(data$Night.Minutes / data$Account.Length, 3)
```

```{r}
# Calculating average monthly day, evening and night calls durations for each plan type
dcalls_monthly_avg <- c(mean(data[data$Plan.Type==1, ]$dcall_avg), mean(data[data$Plan.Type==2, ]$dcall_avg), mean(data[data$Plan.Type==3, ]$dcall_avg), mean(data[data$Plan.Type==4, ]$dcall_avg), mean(data[data$Plan.Type==5, ]$dcall_avg), mean(data[data$Plan.Type==6, ]$dcall_avg), mean(data[data$Plan.Type==7, ]$dcall_avg), mean(data[data$Plan.Type==8, ]$dcall_avg), mean(data[data$Plan.Type==9, ]$dcall_avg), mean(data[data$Plan.Type==10, ]$dcall_avg))

ecalls_monthly_avg <- c(mean(data[data$Plan.Type==1, ]$ecall_avg), mean(data[data$Plan.Type==2, ]$ecall_avg), mean(data[data$Plan.Type==3, ]$ecall_avg), mean(data[data$Plan.Type==4, ]$ecall_avg), mean(data[data$Plan.Type==5, ]$ecall_avg), mean(data[data$Plan.Type==6, ]$ecall_avg), mean(data[data$Plan.Type==7, ]$ecall_avg), mean(data[data$Plan.Type==8, ]$ecall_avg), mean(data[data$Plan.Type==9, ]$ecall_avg), mean(data[data$Plan.Type==10, ]$ecall_avg))

ncalls_monthly_avg <- c(mean(data[data$Plan.Type==1, ]$ncall_avg), mean(data[data$Plan.Type==2, ]$ncall_avg), mean(data[data$Plan.Type==3, ]$ncall_avg), mean(data[data$Plan.Type==4, ]$ncall_avg), mean(data[data$Plan.Type==5, ]$ncall_avg), mean(data[data$Plan.Type==6, ]$ncall_avg), mean(data[data$Plan.Type==7, ]$ncall_avg), mean(data[data$Plan.Type==8, ]$ncall_avg), mean(data[data$Plan.Type==9, ]$ncall_avg), mean(data[data$Plan.Type==10, ]$ncall_avg))
```

```{r}
# Visualizing average monthly day, evening and night calls durations for each plan type
plan_types <- c("1","2","3","4","5","6","7","8","9","10")
colors <- c("dodgerblue1","dodgerblue3","dodgerblue4")
day_time <- c("Day","Evening","Night")

calls_monthly_avg <- matrix(c(dcalls_monthly_avg, ecalls_monthly_avg, ncalls_monthly_avg), nrow = 3, ncol = 10, byrow = TRUE)

barplot(calls_monthly_avg, 
        main = "Average monthly calls duration by plan type", 
        names.arg = plan_types, 
        xlab = "Plan type", 
        ylab = "Average monthly calls duration, min", 
        col = colors)

legend("topright", day_time, cex = 0.6, fill = colors)

```

```{r}
# Stratified random split
set.seed(123)
split_index <- createDataPartition(data$Churn, p = .7, list = FALSE, times = 1)
train <- data[split_index,]
test <- data[-split_index,]

train$Churn <- as.factor(train$Churn)
test$Churn <- as.factor(test$Churn)
```

```{r}
# Creating additional variables to get rid of multicorrelation
data$dcall_dur_avg <- round(data$Day.Minutes / data$Day.Calls, 3)
data$ecall_dur_avg <- round(data$Evening.Minutes / data$Day.Calls, 3)
data$ncall_dur_avg <- round(data$Night.Minutes / data$Day.Calls, 3)
```

```{r}
# Selecting features that will not be used for analysis
dropped_columns <- c("Voice.Mail","Day.Calls","Evening.Calls","Night.Calls","International.Plan.YES","International.Calls","Has.Phone","Phone.Payment.left","dcall_avg","ecall_avg","ncall_avg","dcall_dur_avg","ecall_dur_avg","ncall_dur_avg")

# Dropping unnecessary columns
train <- train[,!(names(train) %in% dropped_columns)]
test <- test[,!(names(test) %in% dropped_columns)]

# Creating dataframe that will later be used to build an ensamble algorithm
data_ens <- data[,!(names(data) %in% dropped_columns)]
```

```{r}
# This formula will be used for all models
formula <- 'Churn ~ Income + Gender + Account.Length + Plan.Type + Voice.Mail.Messages + Day.Minutes + Evening.Minutes + Night.Minutes + International.Minutes + Phone.monthly.payment + Customer.Service.Calls'
```

```{r}
# Logistic regression
logit <- glm(as.formula(formula),family=binomial(link='logit'),data=train)
summary(logit)
```

```{r}
# Trained logistic regression predictions
model_fit <- predict(logit, test, type = 'response')
logit_pred <- ifelse(model_fit > 0.5,1,0)
# Confusion matrix for logistic regression
logit_conf <- confusionMatrix(as.factor(logit_pred), test$Churn, mode = "everything", positive = "1")
logit_conf
logit_f <- logit_conf$byClass[7]
```

```{r}
# Algorithm to tune parameters of support vector machines
# 
# tune_out <- tune.svm(x = train[, -12], y = train$Churn,
#               type = "C-classification",
#               kernel = "polynomial", degree = 2, cost = 10^(-1:1),
#               gamma = c(0.1, 0.5, 1), coef0 = c(0.1, 0.5, 1))
# 
# 
# Obtained optimal values will further be used in svm
# tune_out$best.parameters$cost
# tune_out$best.parameters$gamma
# tune_out$best.parameters$coef0
```

```{r}
set.seed(123)
# Support vector machines
svm_m = svm(formula = as.formula(formula), data = train, scale = TRUE, type = 'C-classification',  kernel = "polynomial", degree = 2, cost = 0.1,  gamma = 1, coef0 = 0.5)
```

```{r}
set.seed(123)
# Support vector machines predictions
svm_p <- predict(svm_m, newdata = test)
svm_conf <- confusionMatrix(svm_p, test$Churn, mode = "everything", positive = "1")
svm_conf
svm_f <- svm_conf$byClass[7]
```
```{r}
# Tuning parameter mtry for random forest
set.seed(123)
train_rf <- as.data.frame(train)
bestmtry <- tuneRF(train_rf[, -12], train_rf[,12], stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry)
```


```{r}
set.seed(123)
# Random forest
randomf = randomForest(as.formula(formula), ntree = 500,  mtry = 3, data = train)
```

```{r}
set.seed(123)
# Random forest predictions
y_pred = predict(randomf, newdata = test[-12])
randf_conf <- confusionMatrix(y_pred, test$Churn, mode = "everything", positive = "1")
importance(randomf)
randf_conf

randf_f <- randf_conf$byClass[7]
```

```{r}
set.seed(123)
# Simple decision treee
dtree <- rpart(formula = as.formula(formula), data = train, cp=0.01)
# Choosing optimal complexity parameter (cp) using plot
printcp(dtree)
plotcp(dtree)

```

```{r}
rpart.plot(dtree)
```

```{r}
set.seed(123)
# Devision tree predictions
tree_pred <- predict(dtree, test, type = 'class')

dtree_conf <- confusionMatrix(tree_pred, test$Churn,  mode = "everything", positive = "1")
dtree_conf
dtree_f <- dtree_conf$byClass[7]
```

```{r}
# Preparing data for xgboost algorithm
xgb_train_churn <- as.matrix(train[12])
xgb_train <- data.matrix(train[-12])

xgb_test_churn <- as.matrix(test[12])
xgb_test <- data.matrix(test[-12])

xgb_train_m = xgb.DMatrix(data=xgb_train, label=xgb_train_churn)
xgb_test_m = xgb.DMatrix(data=xgb_test, label=xgb_test_churn)
```

```{r}
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)
```

```{r}
# Extracting optimal features for xgboost using cross validation
xgbcv <- xgb.cv( params = params, data = xgb_train_m, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = T)
```

```{r}
# XGBoost algorithm
set.seed(123)
xgb <- xgboost(data = xgb_train_m, params=params, nrounds=41)
```

```{r}
set.seed(123)
# xgboost predictions
xgb_p = predict(xgb, xgb_test_m)
xgb_p = as.factor(round(xgb_p))

xgb_conf <- confusionMatrix(xgb_p, test$Churn,  mode = "everything", positive = "1")
xgb_conf
xgb_f <- xgb_conf$byClass[7]
```

```{r}
# Importance of features
xgb.importance(model=xgb)

# SHaP values for each feature
shap <- shap.prep(xgb_model = xgb, X_train = xgb_train)
shap.plot.summary(shap)
```

```{r}
shap.plot.dependence(shap, "Day.Minutes", alpha = 0.7, jitter_width = 0.1)
shap.plot.dependence(shap, "Day.Minutes", color_feature = 'Evening.Minutes', alpha = 0.7, jitter_width = 0.1)
shap.plot.dependence(shap, "Customer.Service.Calls", color_feature = 'Day.Minutes', alpha = 0.7, jitter_width = 0.1)
```


```{r}
shap.plot.dependence(shap, "International.Minutes", color_feature = 'auto', alpha = 0.7, jitter_width = 0.1)
shap.plot.dependence(shap, "Evening.Minutes", color_feature = 'auto', alpha = 0.7, jitter_width = 0.1)
```


```{r}
set.seed(123)
# Predicting churn with each model
log_pred <- predict(logit, newdata = data_ens, type = 'response')
log_pred <- as.factor(ifelse(log_pred > 0.5,1,0))

svm_pred <- predict(svm_m, newdata = data_ens)

randf_pred <- predict(randomf, newdata = data_ens)

dtree_pred <- predict(dtree, newdata = data_ens, type = 'class')

xgb_matrix <- xgb.DMatrix(data=data.matrix(data_ens[,-12]), label=as.matrix(data_ens$Churn))

xgb_pred <- predict(xgb, newdata = xgb_matrix)
xgb_pred <- as.factor(round(xgb_pred))

# Building the dataframe for ensemble method
predictions <- data.frame(log_pred, svm_pred, randf_pred, dtree_pred, xgb_pred)
predictions$ens <- NA
```

```{r}
# Assigning scores for each model based on respective F-score
sum_f <- sum(logit_f, svm_f, dtree_f, randf_f, xgb_f)
log_score <- (logit_f / sum_f)
svm_score <- (svm_f / sum_f)
randf_score <- (randf_f / sum_f)
dtree_score <- (dtree_f / sum_f)
xgb_score <- (xgb_f / sum_f)

scores <- c(log_score, svm_score, randf_score, dtree_score,xgb_score)
```

```{r}
# Ensemble prediction function
for(i in 1:length(log_pred)){
  y = 0
  n = 0
  for (j in 1:5){
    if (predictions[i, j] == 0){
      n = n + scores[j]
    }else{
      y = y + scores[j]
    }
  }
  if (y > n){
    predictions$ens[i] = 1
  }else{
    predictions$ens[i] = 0
  }
}
```

```{r}
# Measuring the performance of an ensemble
ens_conf <- confusionMatrix(as.factor(predictions$ens), as.factor(data_ens$Churn), mode = 'everything', positive = '1')
ens_conf
ens_f <- ens_conf$byClass[7]

f_scores_all <- c(logit_f, svm_f, dtree_f, randf_f, xgb_f, ens_f)
f_scores_all
```

```{r}
# Min max normalization function
minmax_norm <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
```

```{r}
# Dataframe consisting only of people who churned
churn_df <- data_ens[data_ens$Churn==1, ]

churn_df <- churn_df[, c("Day.Minutes", "International.Minutes", "Customer.Service.Calls")]

# churn_df <- as.data.frame(lapply(churn_df, minmax_norm))

# churn_df <- churn_df[,!(names(churn_df) %in% c("Plan.Type", "Gender", "Churn"))]

churn_df <- as.data.frame(scale(churn_df))
```

```{r}
# pca_res = prcomp(churn_df, center = TRUE, scale = TRUE)
# summary(pca_res)
```


```{r}
# Visualizing elbow method

wss <- sapply(1:10, function(k){kmeans(churn_df, k, nstart=25,iter.max = 50 )$tot.withinss})

plot(1:10, wss,
     type="b", pch = 19, frame = F,
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```

```{r}
# Defining optimal number of k for k-means algorithm
set.seed(123)
# Silhouette method
fviz_nbclust(churn_df, kmeans, nstart = 25, k.max = 15, iter.max = 50, method = "silhouette")+
  labs(subtitle = "Silhouette method")

```

```{r}
# K-means with k=4
k4 <- kmeans(churn_df, centers = 4, iter.max = 50, nstart = 25)
k4
```
```{r}
# Evaluating built clusters with silhouette width
sil <- silhouette(k4$cluster, dist(churn_df))
fviz_silhouette(sil)
```

```{r}
churn_df$cluster = factor(k4$cluster)

p <- plot_ly(churn_df, x=~Day.Minutes, y=~International.Minutes, 
z=~Customer.Service.Calls, color=~cluster) %>% add_markers(size=1.5)
print(p)
```

